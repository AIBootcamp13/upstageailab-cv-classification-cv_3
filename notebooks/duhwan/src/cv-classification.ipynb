{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f70981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# 하이드라와 주피터 노트북은 아규먼트 관련 충돌이 발생하므로 초기화 해줌\n",
    "sys.argv = ['']\n",
    "# 환경변수 읽기\n",
    "if (python_path := dotenv_values().get('PYTHONPATH')) and python_path not in sys.path: sys.path.append(python_path)\n",
    "\n",
    "from src.data.FullAugraphyPipeline import FullAugraphyPipeline\n",
    "from src.data.ImageDataset import ImageDataset\n",
    "from src.model.CustomModel import CustomModel\n",
    "from src.util import config\n",
    "\n",
    "# 시드 고정\n",
    "def random_seed(seed_num=42):\n",
    "\n",
    "    \"\"\" SEED = seed_num\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True \"\"\"\n",
    "    \n",
    "    # seed_everything 은 위의 내용 제어 + 밑에내용\n",
    "    pl.seed_everything(seed_num)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 데이터 준비 함수\n",
    "def prepare_data(batch_size=32, num_workers=4):\n",
    "    \n",
    "   # 데이터셋 생성\n",
    "    train_dataset, val_dataset, test_dataset = ImageDataset.get_datasets()\n",
    "\n",
    "    # DataLoader 정의\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # 별도의 검증 데이터셋\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # 검증 시에는 셔플하지 않음\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # model config\n",
    "    model_name = 'resnet34' # 'resnet50' 'efficientnet_b4', ...\n",
    "\n",
    "    # training config\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 0\n",
    "    num_classes = 17\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    # 모델 초기화 전에 설정\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    \n",
    "    random_seed(42)\n",
    "\n",
    "    # 데이터 로더 준비\n",
    "    #train_loader, val_loader, test_loader = prepare_data(batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    augmentation_pipeline = FullAugraphyPipeline(max_effects=3)\n",
    "    \n",
    "    # 커스텀 변환 클래스 정의\n",
    "    class To_BGR(object):\n",
    "        \"\"\"PIL RGB 이미지를 numpy BGR 이미지로 변환\"\"\"\n",
    "        def __call__(self, image):\n",
    "            image_numpy = np.array(image)\n",
    "            if len(image_numpy.shape) < 3:\n",
    "                return cv2.cvtColor(image_numpy, cv2.COLOR_GRAY2BGR)\n",
    "            else:\n",
    "                return cv2.cvtColor(image_numpy, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 수정된 변환 파이프라인\n",
    "    dirty_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        To_BGR(),\n",
    "        augmentation_pipeline,  # Augraphy 적용\n",
    "        transforms.ToTensor(),  # numpy -> tensor\n",
    "        # 정규화\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    clean_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # 정규화\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    d1 = ImageDataset(config.CV_CLS_TRAIN_CSV, config.CV_CLS_TRAIN_DIR, transform=dirty_transforms)\n",
    "    d2 = ImageDataset(config.CV_CLS_TRAIN_CSV, config.CV_CLS_TRAIN_DIR, transform=dirty_transforms)\n",
    "    d3 = ImageDataset(config.CV_CLS_TRAIN_CSV, config.CV_CLS_TRAIN_DIR, transform=clean_transforms)\n",
    "\n",
    "    train_dataset = ConcatDataset([d1, d2, d3])\n",
    "\n",
    "    # 전체 데이터셋을 8:2로 분할\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size]\n",
    "        #generator=torch.Generator().manual_seed(42)  # 재현 가능성을 위한 시드\n",
    "    )\n",
    "\n",
    "    test_dataset = ImageDataset(\n",
    "        config.CV_CLS_TEST_CSV,\n",
    "        config.CV_CLS_TEST_DIR,\n",
    "        transform=clean_transforms\n",
    "    )\n",
    "\n",
    "    # DataLoader 정의\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # 별도의 검증 데이터셋\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,  # 검증 시에는 셔플하지 않음\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    model = CustomModel(\n",
    "        model_name= model_name,\n",
    "        num_classes=num_classes,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "     # 콜백을 직접 생성\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            mode='min',\n",
    "            min_delta=0.001,\n",
    "            verbose=True\n",
    "        ),\n",
    "        LearningRateMonitor(\n",
    "            logging_interval='epoch',\n",
    "            log_momentum=False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    trainer = Trainer(default_root_dir=config.OUTPUTS_DIR, max_epochs=EPOCHS, accelerator='auto', callbacks=callbacks)\n",
    "    \n",
    "    # 훈련\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # 테스트\n",
    "    trainer.test(model, test_loader)\n",
    "\n",
    "    print(\"테스트 갯수=\",len(model.test_predictions))\n",
    "    if len(model.test_predictions) > 0:\n",
    "        # 모든 예측값과 실제값 합치기\n",
    "        all_preds = model.test_predictions\n",
    "        \n",
    "        pred_df = pd.DataFrame(test_loader.dataset.df, columns=['ID', 'target'])\n",
    "        pred_df['target'] = all_preds\n",
    "\n",
    "        sample_submission_df = pd.read_csv(config.CV_CLS_TEST_CSV)\n",
    "        assert (sample_submission_df['ID'] == pred_df['ID']).all()\n",
    "        pred_df.to_csv(config.OUTPUTS_DIR + \"/pred.csv\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"테스트 결과를 가져올 수 없습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/row/train.csv\")\n",
    "value_counts_df = data['target'].value_counts().reset_index(name='count')\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6913999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data2 = pd.read_csv(\"../data/row/meta2.csv\")\n",
    "\n",
    "result = pd.merge(value_counts_df, data2, on='target', how='inner')\n",
    "result.sort_values(by='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def monitor_memory():\n",
    "    process = psutil.Process()\n",
    "    print(f\"메모리 사용량: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "monitor_memory()\n",
    "# 배치 처리 후 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # GPU 메모리 정리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstageailab-cv-classification-cv-3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
